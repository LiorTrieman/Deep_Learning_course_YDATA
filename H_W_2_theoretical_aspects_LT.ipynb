{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X4WWyW4tM2Ir"
      },
      "source": [
        "# Deep Learning Theoretical Aspects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u1ZXXN9lM2Is"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy as sp\n",
        "import sklearn\n",
        "%matplotlib inline\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2qwj-IZM2Iw"
      },
      "source": [
        "Much of the power of neural networks comes from the nonlinearity that is inherited in activation functions.  \n",
        "Show that a network of N layers that uses a linear activation function can be reduced into a network with just an input and output layers.\n",
        "\n",
        "(Write down what is the output of two layers and use induction to claim for all layers).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "jmDcfgIOM2Ix",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "d694ac1f-73a8-4d5a-8113-e6e0ad40cb23"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'each layer in the neural network is a linear combination of the input, the weights and the biases. \\nso if a layer includes only linear activation functions, the activation functio can be replace with a \\nlayer that represents the same linear equation as the activation functions'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "# Write your answer here\n",
        "'''each layer in the neural network is a linear combination of the input, the weights and the biases. \n",
        "so if a layer includes only linear activation functions, the activation functio can be replace with a \n",
        "layer that represents the same linear equation as the activation functions'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "an N-layer neural network with a linear activation function, we can represent the output of each layer as a linear combination of the input:"
      ],
      "metadata": {
        "id": "GGQX8-WQCu0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$z_i^{(l)} = \\sum_{j=1}^{n^{(l-1)}} w_{ij}^{(l)} a_j^{(l-1)}$\n",
        "\n"
      ],
      "metadata": {
        "id": "OKMa__EYComm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "where $z_i^{(l)}$ is the input to the i-th neuron in the l-th layer, $n^{(l-1)}$ is the number of neurons in the (l-1)-th layer, $w_{ij}^{(l)}$ is the weight connecting the j-th neuron in the (l-1)-th layer to the i-th neuron in the l-th layer, and $a_j^{(l-1)}$ is the output of the j-th neuron in the (l-1)-th layer.\n",
        "\n",
        "Since the activation function is linear, we can simply replace it with an identity function. This means that the output of the l-th layer is simply:\n",
        "\n",
        "$a_i^{(l)} = z_i^{(l)}$\n",
        "\n",
        "Now, we can substitute the expression for $z_i^{(l)}$ into the expression for $a_i^{(l)}$ to get:\n",
        "\n",
        "$a_i^{(l)} = \\sum_{j=1}^{n^{(l-1)}} w_{ij}^{(l)} a_j^{(l-1)}$\n",
        "\n",
        "We can repeat this process for all layers until we get to the output layer:\n",
        "\n",
        "$a_i^{(N)} = \\sum_{j=1}^{n^{(N-1)}} w_{ij}^{(N)} a_j^{(N-1)}$\n",
        "\n",
        "At this point, we have a network with just an input and output layer. The input layer has $n^{(0)}$ neurons, and the output layer has $n^{(N)}$ neurons. The weights connecting the input layer to the output layer are given by $w_{ij}^{(N)}$. Therefore, we have reduced the original N-layer neural network into a single-layer neural network."
      ],
      "metadata": {
        "id": "ltNJHnKWCwXD"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-e-VlB4eM2Iz"
      },
      "source": [
        "### Derivatives of Activation Functions\n",
        "Compute the derivative of these activation functions:\n",
        "\n",
        "1 Sigmoid\n",
        "<img src=\"https://cdn-images-1.medium.com/max/1200/1*Vo7UFksa_8Ne5HcfEzHNWQ.png\" width=\"150\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "bk-7BzFQM2I0"
      },
      "outputs": [],
      "source": [
        "# Write your answer here\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 1/(1 + np.exp(-x))\n",
        "\n",
        "def sigmoid_derivative(x):\n",
        "  sig = sigmoid(x)\n",
        "  return sig*(1 - sig)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "$f'(x) = \\frac{d}{dx} \\frac{1}{1+e^{-x}}$ $ = \\frac{d}{dx} (1+e^{-x})^{-1}$\n",
        "\n",
        "Using the chain rule, we can express this as:\n",
        "\n",
        "\n",
        "\n",
        "$f'(x) = -(1+e^{-x})^{-2} \\cdot \\frac{d}{dx} (1+e^{-x})$\n",
        "\n",
        "$f'(x) = \\frac{e^{-x}}{(1+e^{-x})^{2}}$\n",
        "\n",
        "$f'(x) = \\frac{1}{1+e^{-x}} \\cdot \\frac{e^{-x}}{1+e^{-x}}$\n",
        "\n",
        "$f'(x) = f(x) \\cdot \\frac{e^{-x}}{1+e^{-x}}$\n",
        "\n",
        "$f'(x) = f(x) \\cdot (1-f(x))$\n",
        "\n",
        "Therefore, the derivative of the sigmoid function is:\n",
        "\n",
        "$f'(x) = f(x) \\cdot (1-f(x))$"
      ],
      "metadata": {
        "id": "Wsw0AXWTDiTR"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0AiF6YjM2I3"
      },
      "source": [
        "2 Relu \n",
        "\n",
        "<img src=\"https://cloud.githubusercontent.com/assets/14886380/22743194/73ca0834-ee54-11e6-903f-a7efd247406b.png\" width=\"200\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BWTRtEX8M2I4"
      },
      "outputs": [],
      "source": [
        "# Write your answer here\n",
        "\n",
        "def ReLu_derivative(x):\n",
        "  if x <= 0:\n",
        "    return 0\n",
        "  else:\n",
        "    return 1\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To find its derivative, we need to differentiate between two cases: when the input is positive (greater than or equal to zero) and when the input is negative.\n",
        "\n",
        "When $x \\geq 0$, the function is just $f(x) = x$. Therefore, its derivative is:\n",
        "\n",
        "$f'(x) = \\frac{d}{dx} x = 1$\n",
        "\n",
        "When $x < 0$, the function is just $f(x) = 0$. Therefore, its derivative is:\n",
        "\n",
        "$f'(x) = \\frac{d}{dx} 0 = 0$\n",
        "\n",
        "Putting these cases together, the derivative of the ReLU function is:\n",
        "\n",
        "$f'(x) = \\begin{cases}\n",
        "0 & x < 0 \\\n",
        "1 & x \\geq 0\n",
        "\\end{cases}$\n",
        "\n",
        "This derivative is not defined at $x = 0$, but we can choose to define it as either 0 or 1 using the convention of subgradient."
      ],
      "metadata": {
        "id": "kFgmq0aqElku"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8tcbCKStM2I7"
      },
      "source": [
        "3 Softmax\n",
        "<img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e348290cf48ddbb6e9a6ef4e39363568b67c09d3\" width=\"250\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "0Qb8zeNBM2I8"
      },
      "outputs": [],
      "source": [
        "# Write your answer here\n",
        "def Softmax(z):\n",
        "  z_sum = np.sum(z)\n",
        "  return z/z_sum\n",
        "\n",
        "def delta(i,j):\n",
        "  if i == j:\n",
        "    return 1\n",
        "  else:\n",
        "    return 0\n",
        "\n",
        "def softmax_derivative(z):\n",
        "  n = len(z)\n",
        "  DS = np.zeros((n, n))\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      DS[i][j] = Softmax(z[i])*(delta(i,j) - Softmax(z[j]))\n",
        "  return DS"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Softmax activation function is defined as:\n",
        "\n",
        "$f_j(z) = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}$ for $j=1,2,\\ldots,K$\n",
        "\n",
        "where $K$ is the number of output classes.\n",
        "\n",
        "To find the derivative of the Softmax function, we need to compute the partial derivative of $f_j(z)$ with respect to $z_k$ for all $j$ and $k$. Using the quotient rule, we can derive the expression as follows:\n",
        "\n",
        "$\\frac{\\partial f_j}{\\partial z_k} = \\frac{\\frac{\\partial}{\\partial z_k} e^{z_j} \\sum_{k=1}^{K} e^{z_k} - e^{z_j} \\frac{\\partial}{\\partial z_k} \\sum_{k=1}^{K} e^{z_k}}{(\\sum_{k=1}^{K} e^{z_k})^2}$\n",
        "\n",
        "If $j = k$, we can simplify this expression to:\n",
        "\n",
        "$\\frac{\\partial f_j}{\\partial z_k} = \\frac{e^{z_j} (\\sum_{k=1}^{K} e^{z_k} - e^{z_j})}{(\\sum_{k=1}^{K} e^{z_k})^2}$\n",
        "\n",
        "$\\frac{\\partial f_j}{\\partial z_k} = f_j(z) (1 - f_j(z))$ for $j = k$\n",
        "\n",
        "If $j \\neq k$, we can simplify the expression to:\n",
        "\n",
        "$\\frac{\\partial f_j}{\\partial z_k} = \\frac{- e^{z_j} e^{z_k}}{(\\sum_{k=1}^{K} e^{z_k})^2}$\n",
        "\n",
        "$\\frac{\\partial f_j}{\\partial z_k} = -f_j(z) f_k(z)$ for $j \\neq k$\n",
        "\n",
        "Therefore, the derivative of the Softmax function is:\n",
        "\n",
        "$\\frac{\\partial f_j}{\\partial z_k} = \\begin{cases}\n",
        "f_j(z) (1 - f_j(z)) & i = k \\\n",
        "-f_j(z) f_k(z) & j \\neq k\n",
        "\\end{cases}$\n",
        "\n",
        "This can also be written in matrix form."
      ],
      "metadata": {
        "id": "-m7gGffkFZLZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRE-pv-zM2I-"
      },
      "source": [
        "### Back Propagation\n",
        "Use the chain rule and backprop (also called the generalized delta rule) to compute the partial derivatives for these computations (i.e., dz/dx1, dz/dx1, dz/dx3):"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sJZ_0mWM2JA"
      },
      "source": [
        "```\n",
        "z = x1 + 5*x2 - 3*x3^2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "yPXdNKsGM2JA"
      },
      "outputs": [],
      "source": [
        "# Write your answer here, using Markdown, image or any other suitable format"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x_1}  = 1 \\cdot 1 = 1$\n",
        "\n",
        "\n",
        "Similarly, we have:\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x_2} = 1 \\cdot 5 = 5$\n",
        "\n",
        "\n",
        "\n",
        "Finally, we have:\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x_3} = 1 \\cdot (-6x_3) = -6x_3$\n",
        "\n"
      ],
      "metadata": {
        "id": "51adsMrjFk7d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pgwnBRJgM2JD"
      },
      "source": [
        "```\n",
        "z = x1*(x2-4) + exp(x3^2) / 5*x4^2\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQ1igYpxM2JE"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the chain rule, we can compute the partial derivatives of the function $z = x_1(x_2 - 4) + \\frac{\\exp(x_3^2)}{5x_4^2}$ as follows:\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x_1} = x_2 - 4$\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x_2} = x_1$\n",
        "\n",
        "\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x_3} = 2x_3\\frac{\\exp(x_3^2)}{5x_4^2}$\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x_4} = -\\frac{2\\exp(x_3^2)}{5x_4^3}$\n",
        "\n"
      ],
      "metadata": {
        "id": "F3nCGUW6F4oC"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCIx61WAM2JI"
      },
      "source": [
        "```\n",
        "z = 1/x3 + exp( (x1+5*(x2+3)) ^2 )\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zSdvzQTlM2JJ"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using the chain rule, we can compute the partial derivatives of the function $z = \\frac{1}{x_3} + \\exp((x_1+5(x_2+3))^2)$ as follows:\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x_1} = 2(x_1+5(x_2+3))\\exp((x_1+5(x_2+3))^2)$\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x_2} = 10(x_1+5(x_2+3))\\exp((x_1+5(x_2+3))^2)$\n",
        "\n",
        "$\\frac{\\partial z}{\\partial x_3} = -\\frac{1}{x_3^2}$\n",
        "\n"
      ],
      "metadata": {
        "id": "HUvwItp6GfjB"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvTBLm44M2Jq"
      },
      "source": [
        "### Puppy or bagel?\n",
        "We've seen in class the (hopefully) funny examples of challenging images (Chihuahua or muffin, puppy or bagel etc.). \n",
        "\n",
        "Let's say you were asked by someone to find more examples like that. You are able to call the 3 neural networks that won the recent ImageNet challenges, and get their predictions (the entire vector of probabilities for the 1000 classes).  \n",
        "\n",
        "Describe methods that might assist you in finding more examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsTYNPDvM2Jr"
      },
      "outputs": [],
      "source": [
        "# Write your answer here"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following methods might help in finding similar examples:\n",
        "\n",
        "1) Adversarial Examples:  adding small, imperceptible perturbations to an image that cause the model to misclassify it. This method can be used to create images that are difficult to classify and could be mistaken for other objects.\n",
        "\n",
        "2) Image augmentation:adding variations to the original image to increase the size of the training set. By applying different transformations such as rotations, scaling, and cropping, it is possible to generate images that are similar to the original but difficult to classify. These images can be used to test the robustness of the model and identify areas for improvement.\n",
        "\n",
        "3) Data collection: collecting real world data with similar missleading examples.\n",
        "\n",
        "4) Feature visualization: Feature visualization techniques can help identify what a deep neural network is looking for in an image. This can be used to identify images that have similar features but are different objects. For example, images of Chihuahuas and muffins might have similar features that make them difficult to distinguish, such as round shapes and similar coloration."
      ],
      "metadata": {
        "id": "zJJsxOHZHAIY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convolusion\n",
        "Consider the following convolution filters:\n",
        "```python\n",
        "k1 = [ [0 0 0], [0 1 0], [0 0 0] ]\n",
        "k2 = [ [0 0 0], [0 0 1], [0 0 0] ]\n",
        "k3 = [ [-1-1 -1], [-1 8 -1], [-1 -1 -1] ]\n",
        "k4 = [ [1 1 1], [1 1 1], [1 1 1] ] / 9\n",
        "```\n",
        "\n",
        "Can you guess what each of them computes?"
      ],
      "metadata": {
        "id": "-JF9a_kQdqlk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your answer here"
      ],
      "metadata": {
        "id": "CzP36pi3eGvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each of the convolution filters has a different purpose and computes different operations on an input image:\n",
        "\n",
        "k1 = [ [0 0 0], [0 1 0], [0 0 0] ]\n",
        "This filter is called an identity filter because it passes the input image through unchanged. It assigns a weight of 1 to the center pixel and 0 to all other pixels. \n",
        "\n",
        "k2 = [ [0 0 0], [0 0 1], [0 0 0] ]\n",
        "This filter is  called a horizontal edge detection filter. It assigns a weight of 1 to the middle-right pixel and a weight of -1 to the middle-left pixel. It detects edges that are oriented horizontally.\n",
        "\n",
        "k3 = [ [-1 -1 -1], [-1 8 -1], [-1 -1 -1] ]\n",
        "This filter is called a Laplacian filter or edge enhancement filter. It detects edges by computing the second derivative of the image intensity. It assigns a weight of -1 to all eight neighboring pixels and a weight of 8 to the center pixel. It enhances edges and reduces noise.\n",
        "\n",
        "k4 = [ [1 1 1], [1 1 1], [1 1 1] ] / 9\n",
        "This filter is called a mean filter or smoothing filter. It assigns a weight of 1/9 to each of the nine neighboring pixels and computes the average value. It smooths the image and reduces noise."
      ],
      "metadata": {
        "id": "bQs5Z0WNHOAT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "O4EIsgf3HORx"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}